\section{HARP extension for flexible performance-complexity balancing}\label{sec:our-method}

While the previously described style of prolongation is sufficient when HARP is used only as a means of pre-training, this approach is far too crude when studying the relationship between graph complexity and the quality of graph embedding and subsequent downstream applications as one coarsening iteration can reduce the number of nodes to less than half. Such a reduction ratio effectively prevents any sufficient understanding of the relationship between graph reduction and changes in the quality of its embedding. In order to overcome this, we present the adaptive prolongation approach. This algorithm works with the pre-coarsened graphs produced by HARP, however, the embedding is learned in a different manner.

The adaptive prolongation approach decouples the \( L \) coarsening steps from \( K \) prolongation steps, where \( K \) is independent of \( L \). The prolongation steps are then driven by local properties of the graph with relation to the downstream task, allowing for different levels of granularity in different parts of the graph.

Let us denote \( \Psi_K, \dots, \Psi_0 \) the resulting embedding sequence. Similarly to standard HARP prolongation, the algorithm starts with the coarsest graph \( G_L \), trains a graph model to compute its embedding \( \Psi_K \) and gradually refines it until reaching the embedding \( \Psi_0 \). Instead of directly setting the value of \( K \), a fixed number of nodes \( n_p \) is prolonged in each step. These prolongation steps are interlaid with continued training of the graph model, as in standard HARP. A description of a single prolongation step from \( \Psi_{i + 1} \) to \( \Psi_i \) follows.

The procedure keeps track of all the edge contractions that were made in the dataset augmentation part of the algorithm and gradually reverses them. To this end, apart from the embedding \( \Psi_i \), the set of all contractions yet to be reversed as of step \( i \) is kept as \( \mathcal{C}_L^{(i)}, \dots, \mathcal{C}_0^{(i)} \), with the initial values \( \mathcal{C}_j^{(K)} \) corresponding to the underlying coarsening \( \varphi_j \) as defined in Section~\ref{sec:harp}.

In each prolongation step, the embedding \( \Psi_{i + 1} \) is prolonged to \( \Psi_i \) by selecting a set of \( n_p \) contractions \( \mathcal{C}_\mathrm{prolong} \) from the original coarsening procedure and undoing them by copying and reusing the embedding of the node resulting from the contraction to both of the nodes that were contracted. To obtain this set of contractions, nodes of \( G_0 \) are first ordered in such a way that corresponds to the usefulness of prolonging them. Subsequently, the set \( \mathcal{C}_\mathrm{prolong} \) is selected from \( \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} \) by selecting contractions affecting nodes in the aforementioned order, until \( n_p \) contractions are selected. If multiple contractions affecting the same node are available in the sequence \( \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} \), one is selected from \( \mathcal{C}_j^{(i + 1)} \) corresponding to the coarsest-level coarsening, that is, from \( \mathcal{C}_j^{(i + 1)} \) with the highest possible \( j \). The sequence \( \mathcal{C}_L^{(i)}, \dots, \mathcal{C}_0^{(i)} \) is produced from \( \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} \) by removing all of the edges contained in \( \mathcal{C}_\mathrm{prolong} \) (each edge from \( \mathcal{C}_\mathrm{prolong} \) will be contained in exactly one of \( \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} \)).

To obtain an ordering of nodes of \( G_0 \) based on the usefulness of their prolongation, the embedding \( \Psi_{i + 1} \) is fully prolonged to a temporary embedding of the full graph, \( \Psi_0^\mathrm{temp} \). The downstream model is then trained using this temporary embedding to obtain \( \mathmat{Y}_\mathrm{pred} \), the predicted posterior distribution of classes for each node in \( G_0 \). The nodes are then ordered by the entropy of the posterior, making the algorithm prolong the nodes where the classifier is the most uncertain.
