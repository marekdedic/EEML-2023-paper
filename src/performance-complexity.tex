\section{The performance-complexity trade-off problem}\label{sec:performance-complexity}

The main aim of this work is to explore the performance-complexity characteristics in the context of graph learning, as introduced in \cite{prochazka_downstream_2022}. Consider an undirected graph \( G \) with nodes \( V \left( G \right) \) and edges \( E \left( G \right) \). The result of the graph coarsening part of the algorithm is a sequence of graphs \( G_0, G_1, G_2, \dots, G_L \) where \( G_0 = G \) and \( L \in \mathfield{N} \) is a hyper-parameter of the method.
Given a model \( M \) that operates on graphs, a performance metric, and a complexity metric, the sequence \( G_0, G_1, \dots, G_L \) corresponds to points in the performance-complexity plane, where advancing along the sequence generally hurts performance and decreases complexity.

This performance-complexity characteristic allows for a choice of an optimal \textbf{working point} for the particular use-case. In this work, the transductive node classification accuracy on a testing dataset is chosen as the performance metric. For the complexity metric, the number of nodes in the graph was chosen as it constitutes a good proxy for real-world algorithmic complexity, as shown in \cite{chiang_cluster-gcn_2019}. The methods proposed in the rest of this work evaluate the graphs in reverse order, i.e. starting with the simplest one. This allows for the choice of the working point with lower complexity compared to the original. Further discussion of performance and complexity metrics, their suitability and the practical usefulness is considered in \cite{prochazka_downstream_2022}.
